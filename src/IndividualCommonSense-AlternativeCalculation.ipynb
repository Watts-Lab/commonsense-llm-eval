{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b983dadb",
   "metadata": {},
   "source": [
    "## Common Sense: Individual Level\n",
    "\n",
    "Slightly different calculation: use the model's rating in the calculation of the majority vote as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd95a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to home directory\n",
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68657b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib\n",
    "\n",
    "# Set default font to Arial\n",
    "matplotlib.rcParams[\"font.family\"] = \"Arial\"\n",
    "matplotlib.rcParams[\"font.sans-serif\"] = \"Arial\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf59ec31",
   "metadata": {},
   "source": [
    "## Statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acc09892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load statements\n",
    "statements = pd.read_csv(\"data/statements_and_prompts.csv\")\n",
    "statements = statements[\"statement\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db21a9",
   "metadata": {},
   "source": [
    "## Common Sense of Humans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b119f",
   "metadata": {},
   "source": [
    "### Load Human Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc0ab7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human ratings\n",
    "# Do you agree with this statement?\n",
    "individual = pd.read_csv(\"data/results/individual_ratings.csv\", index_col=0)\n",
    "\n",
    "# Do you think most people would agree with this statement?\n",
    "group = pd.read_csv(\"data/results/group_ratings.csv\", index_col=0)\n",
    "\n",
    "# Majority voting (skip participants who didn't answer)\n",
    "avg_vote_per_q = individual.mean(axis=1, skipna=True)\n",
    "maj_i = (avg_vote_per_q >= 0.5).astype(int)\n",
    "\n",
    "avg_vote_per_q_others = group.mean(axis=1, skipna=True)\n",
    "\n",
    "# Majority of \"others agree\" answers by humans\n",
    "maj_others = (group.mean(1, skipna=True) >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c342f",
   "metadata": {},
   "source": [
    "### Statement-Level Commonsensicality of Humans\n",
    "\n",
    "This calculation uses **human ratings** only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d097d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statement: consensus\n",
    "c_i = 2 * np.abs(individual.mean(1) - 0.5)\n",
    "\n",
    "# Statement: awareness\n",
    "a_i = pd.Series(\n",
    "    [(group.loc[i].dropna().astype(int) == maj_i[i]).mean() for i in maj_i.index],\n",
    "    index=maj_i.index,\n",
    ")\n",
    "\n",
    "# Statement: commonsensicality\n",
    "m_i = np.sqrt(c_i * a_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38af62e8",
   "metadata": {},
   "source": [
    "### Individual-Level Commonsensicality of Humans\n",
    "\n",
    "This calculation uses **human ratings** only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "febe9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person: consensus\n",
    "C_j = pd.Series(\n",
    "    [\n",
    "        (\n",
    "            individual.loc[:, j].dropna().astype(int)\n",
    "            == maj_i.loc[individual.loc[:, j].dropna().index]\n",
    "        ).mean()\n",
    "        for j in individual.columns\n",
    "    ],\n",
    "    index=individual.columns,\n",
    ")\n",
    "\n",
    "# Person: awareness\n",
    "A_j = pd.Series(\n",
    "    [\n",
    "        (\n",
    "            group.loc[:, j].dropna().astype(int)\n",
    "            == maj_i.loc[group.loc[:, j].dropna().index]\n",
    "        ).mean()\n",
    "        for j in group.columns\n",
    "    ],\n",
    "    index=group.columns,\n",
    ")\n",
    "\n",
    "# Person: commonsensicality\n",
    "M_j = np.sqrt(C_j * A_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def85fb",
   "metadata": {},
   "source": [
    "## Common Sense of LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e0271",
   "metadata": {},
   "source": [
    "### Load Model Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1971dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_color = {\n",
    "    \"GPT-3.5\": \"forestgreen\",\n",
    "    \"GPT-4-0125\": \"forestgreen\",\n",
    "    \"GPT-4-0409\": \"forestgreen\",\n",
    "    \"GPT-4o\": \"forestgreen\",\n",
    "    \"GPT-5\": \"forestgreen\",\n",
    "    \"LLaMA-2-7B\": \"indianred\",\n",
    "    \"LLaMA-2-13B\": \"indianred\",\n",
    "    \"LLaMA-2-70B\": \"indianred\",\n",
    "    \"LLaMA-3-8B\": \"indianred\",\n",
    "    \"LLaMA-3-70B\": \"indianred\",\n",
    "    \"Flan-T5-Small\": \"royalblue\",\n",
    "    \"Flan-T5-Base\": \"royalblue\",\n",
    "    \"Flan-T5-Large\": \"royalblue\",\n",
    "    \"Flan-T5-XL\": \"royalblue\",\n",
    "    \"Flan-T5-XXL\": \"royalblue\",\n",
    "    \"Gemma-2B\": \"chocolate\",\n",
    "    \"Gemma-7B\": \"chocolate\",\n",
    "    \"Gemini Pro 1.0\": \"chocolate\",\n",
    "    \"Mistral-7B\": \"darkviolet\",\n",
    "    \"Mixtral-8x7B\": \"darkviolet\",\n",
    "    \"Mixtral-8x22B\": \"darkviolet\",\n",
    "    \"Mistral-Large\": \"darkviolet\",\n",
    "    \"OLMo-7B\": \"goldenrod\",\n",
    "    \"Falcon-7B\": \"teal\",\n",
    "    \"Falcon-40B\": \"teal\",\n",
    "    \"Falcon-180B\": \"teal\",\n",
    "    \"Claude 3 Haiku\": \"olivedrab\",\n",
    "    \"Claude 3 Sonnet\": \"olivedrab\",\n",
    "    \"Claude 3 Opus\": \"olivedrab\",\n",
    "    \"DBRX\": \"crimson\",\n",
    "    \"Qwen2-0.5B\": \"lightseagreen\",\n",
    "    \"Qwen2-1.5B\": \"lightseagreen\",\n",
    "    \"Qwen2-7B\": \"lightseagreen\",\n",
    "    \"Qwen2-57B\": \"lightseagreen\",\n",
    "    \"Qwen2-72B\": \"lightseagreen\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "099f2606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q1 has 50 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q2 has 50 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q3 has 50 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q1 has 23 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q2 has 23 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q3 has 23 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q1 has 23 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q2 has 23 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q3 has 23 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q1 has 23 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q2 has 23 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q3 has 23 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q1 has 23 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q2 has 23 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question q3 has 23 repetitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    }
   ],
   "source": [
    "from src.utilities import load_annotations_gpt\n",
    "from src.utilities import load_results_hf\n",
    "from src.utilities import load_results_freq\n",
    "from src.utilities import load_results_gpt5\n",
    "\n",
    "all_models = {}\n",
    "\n",
    "all_models[\"GPT-3.5\"] = load_annotations_gpt(\n",
    "    model_name=\"gpt-3.5-turbo-0125\", trial_no=1, verbose=True\n",
    ")\n",
    "all_models[\"GPT-4-0125\"] = load_annotations_gpt(\n",
    "    model_name=\"gpt-4-0125-preview\", trial_no=1, verbose=True\n",
    ")\n",
    "all_models[\"GPT-4-0409\"] = load_annotations_gpt(\n",
    "    model_name=\"gpt-4-turbo-2024-04-09\", trial_no=1, verbose=True\n",
    ")\n",
    "all_models[\"GPT-4o\"] = load_annotations_gpt(\n",
    "    model_name=\"gpt-4o-2024-05-13\", trial_no=1, verbose=True\n",
    ")\n",
    "all_models[\"GPT-5\"] = load_results_gpt5(model_name=\"gpt-5-2025-08-07\", verbose=True)\n",
    "\n",
    "all_models[\"LLaMA-2-7B\"] = load_results_hf(\n",
    "    model_name=\"meta-llama--Llama-2-7b-chat-hf\", verbose=True\n",
    ")\n",
    "all_models[\"LLaMA-2-13B\"] = load_results_hf(\n",
    "    model_name=\"meta-llama--Llama-2-13b-chat-hf\", verbose=True\n",
    ")\n",
    "all_models[\"LLaMA-2-70B\"] = load_results_hf(\n",
    "    model_name=\"meta-llama--Llama-2-70b-chat-hf\", verbose=True\n",
    ")\n",
    "\n",
    "all_models[\"LLaMA-3-8B\"] = load_results_hf(\n",
    "    model_name=\"meta-llama--Meta-Llama-3-8B-Instruct\", verbose=True\n",
    ")\n",
    "all_models[\"LLaMA-3-70B\"] = load_results_hf(\n",
    "    model_name=\"meta-llama--Meta-Llama-3-70B-Instruct\", verbose=True\n",
    ")\n",
    "\n",
    "all_models[\"Flan-T5-Small\"] = load_results_hf(\n",
    "    model_name=\"google--flan-t5-small\", verbose=True\n",
    ")\n",
    "all_models[\"Flan-T5-Base\"] = load_results_hf(\n",
    "    model_name=\"google--flan-t5-base\", verbose=True\n",
    ")\n",
    "all_models[\"Flan-T5-Large\"] = load_results_hf(\n",
    "    model_name=\"google--flan-t5-large\", verbose=True\n",
    ")\n",
    "all_models[\"Flan-T5-XL\"] = load_results_hf(\n",
    "    model_name=\"google--flan-t5-xl\", verbose=True\n",
    ")\n",
    "all_models[\"Flan-T5-XXL\"] = load_results_hf(\n",
    "    model_name=\"google--flan-t5-xxl\", verbose=True\n",
    ")\n",
    "\n",
    "all_models[\"Gemma-2B\"] = load_results_hf(model_name=\"google--gemma-2b-it\", verbose=True)\n",
    "all_models[\"Gemma-7B\"] = load_results_hf(model_name=\"google--gemma-7b-it\", verbose=True)\n",
    "\n",
    "all_models[\"Gemini Pro 1.0\"] = load_results_freq(model_name=\"gemini-pro\", verbose=True)\n",
    "\n",
    "all_models[\"Mistral-7B\"] = load_results_hf(\n",
    "    model_name=\"mistralai--Mistral-7B-Instruct-v0.2\", verbose=True\n",
    ")\n",
    "all_models[\"Mixtral-8x7B\"] = load_results_hf(\n",
    "    model_name=\"mistralai--Mixtral-8x7B-Instruct-v0.1\", verbose=True\n",
    ")\n",
    "all_models[\"Mixtral-8x22B\"] = load_results_hf(\n",
    "    model_name=\"mistralai--Mixtral-8x22B-Instruct-v0.1\", verbose=True\n",
    ")\n",
    "\n",
    "all_models[\"Mistral-Large\"] = load_results_freq(\n",
    "    model_name=\"mistral-large-latest\", verbose=True\n",
    ")\n",
    "\n",
    "all_models[\"OLMo-7B\"] = load_results_hf(\n",
    "    model_name=\"allenai--OLMo-7B-Instruct\", verbose=True\n",
    ")\n",
    "\n",
    "all_models[\"Falcon-7B\"] = load_results_hf(\n",
    "    model_name=\"tiiuae--falcon-7b-instruct\", verbose=True\n",
    ")\n",
    "all_models[\"Falcon-40B\"] = load_results_hf(\n",
    "    model_name=\"tiiuae--falcon-40b-instruct\", verbose=True\n",
    ")\n",
    "all_models[\"Falcon-180B\"] = load_results_hf(\n",
    "    model_name=\"tiiuae--falcon-180B-chat\", verbose=True\n",
    ")\n",
    "\n",
    "all_models[\"Claude 3 Opus\"] = load_results_freq(\n",
    "    model_name=\"claude-3-opus\", verbose=True\n",
    ")\n",
    "all_models[\"Claude 3 Sonnet\"] = load_results_freq(\n",
    "    model_name=\"claude-3-sonnet\", verbose=True\n",
    ")\n",
    "all_models[\"Claude 3 Haiku\"] = load_results_freq(\n",
    "    model_name=\"claude-3-haiku\", verbose=True\n",
    ")\n",
    "\n",
    "all_models[\"DBRX\"] = load_results_hf(\n",
    "    model_name=\"databricks--dbrx-instruct\", verbose=True\n",
    ")\n",
    "\n",
    "all_models[\"Qwen2-0.5B\"] = load_results_hf(\n",
    "    model_name=\"Qwen--Qwen2-0.5B-Instruct\", verbose=True\n",
    ")\n",
    "all_models[\"Qwen2-1.5B\"] = load_results_hf(\n",
    "    model_name=\"Qwen--Qwen2-1.5B-Instruct\", verbose=True\n",
    ")\n",
    "all_models[\"Qwen2-7B\"] = load_results_hf(\n",
    "    model_name=\"Qwen--Qwen2-7B-Instruct\", verbose=True\n",
    ")\n",
    "all_models[\"Qwen2-57B\"] = load_results_hf(\n",
    "    model_name=\"Qwen--Qwen2-57B-A14B-Instruct\", verbose=True\n",
    ")\n",
    "all_models[\"Qwen2-72B\"] = load_results_hf(\n",
    "    model_name=\"Qwen--Qwen2-72B-Instruct\", verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "153049ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model votes\n",
    "def get_model_probs(model_name, q=\"q1\"):\n",
    "\n",
    "    q_answers = all_models[model_name][q]\n",
    "    q_answers = q_answers[[\"yes\", \"no\", \"other\"]]\n",
    "\n",
    "    q_answers = q_answers.to_numpy()\n",
    "    other = q_answers[:, 2]\n",
    "    q_answers[:, 0] += other / 2\n",
    "    q_answers[:, 1] += other / 2\n",
    "\n",
    "    # Ignore probability mass of \"other\" option\n",
    "    # Ensure that the probabilities of \"yes\" and \"no\" sum to 1\n",
    "    q_answers = q_answers[:, 0:2]\n",
    "    q_answers /= q_answers.sum(1, keepdims=True)\n",
    "\n",
    "    q_answers = pd.DataFrame(q_answers, columns=[\"yes\", \"no\"], index=individual.index)\n",
    "    return q_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c5866b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "yes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "no",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e902f3ce-ed9a-4887-9247-84bb47141a1f",
       "rows": [
        [
         "0",
         "1.0",
         "0.0"
        ],
        [
         "1",
         "0.6666666666666666",
         "0.3333333333333333"
        ],
        [
         "2",
         "0.9545454545454546",
         "0.045454545454545456"
        ],
        [
         "3",
         "1.0",
         "0.0"
        ],
        [
         "4",
         "1.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yes</th>\n",
       "      <th>no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        yes        no\n",
       "0  1.000000  0.000000\n",
       "1  0.666667  0.333333\n",
       "2  0.954545  0.045455\n",
       "3  1.000000  0.000000\n",
       "4  1.000000  0.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human votes\n",
    "humans_agree = (individual == 1).sum(1, skipna=True)\n",
    "humans_disagree = (individual == 0).sum(1, skipna=True)\n",
    "humans_q1_soft = np.vstack((humans_agree, humans_disagree), dtype=float).T\n",
    "humans_q1_soft /= humans_q1_soft.sum(1, keepdims=True)\n",
    "\n",
    "humans_q1_soft = pd.DataFrame(\n",
    "    humans_q1_soft, columns=[\"yes\", \"no\"], index=individual.index\n",
    ")\n",
    "\n",
    "humans_others_agree = (group == 1).sum(1, skipna=True)\n",
    "humans_others_disagree = (group == 0).sum(1, skipna=True)\n",
    "humans_q2_soft = np.vstack((humans_others_agree, humans_others_disagree), dtype=float).T\n",
    "humans_q2_soft /= humans_q2_soft.sum(1, keepdims=True)\n",
    "\n",
    "humans_q2_soft = pd.DataFrame(\n",
    "    humans_q2_soft, columns=[\"yes\", \"no\"], index=individual.index\n",
    ")\n",
    "humans_q2_soft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "250e8507",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_q1_probs = {}\n",
    "for model_name in all_models.keys():\n",
    "    all_models_q1_probs[model_name] = get_model_probs(model_name, \"q1\")\n",
    "\n",
    "all_models_q2_probs = {}\n",
    "for model_name in all_models.keys():\n",
    "    all_models_q2_probs[model_name] = get_model_probs(model_name, \"q2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a965109d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "yes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "no",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "other",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "diff",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a1eae655-627d-4a96-b366-b61a189dd167",
       "rows": [
        [
         "3569",
         "0.4998673062182844",
         "0.49986734425537366",
         "0.00026534952634194085",
         "3.803708925609328e-08"
        ],
        [
         "1529",
         "0.4999913003168307",
         "0.4999912356915383",
         "1.7463991631015876e-05",
         "6.462529239525949e-08"
        ],
        [
         "3362",
         "0.4999973469429972",
         "0.49999728120499387",
         "5.371852008936849e-06",
         "6.573800331866053e-08"
        ],
        [
         "1886",
         "0.49998956910418363",
         "0.4999895000930648",
         "2.0930802751553928e-05",
         "6.901111881951749e-08"
        ],
        [
         "2337",
         "0.49998969205106836",
         "0.49998960970601536",
         "2.069824291623615e-05",
         "8.234505299853367e-08"
        ],
        [
         "2221",
         "0.49995499754316564",
         "0.49995490820684707",
         "9.009424998722155e-05",
         "8.933631856500313e-08"
        ],
        [
         "3510",
         "0.4999957495637372",
         "0.49999564240437877",
         "8.608031883963863e-06",
         "1.0715935844229918e-07"
        ],
        [
         "3469",
         "0.49999951212266686",
         "0.4999993864784524",
         "1.101398880681878e-06",
         "1.2564421447702756e-07"
        ],
        [
         "1492",
         "0.49999833726158216",
         "0.49999820666727396",
         "3.4560711439089526e-06",
         "1.3059430820083762e-07"
        ],
        [
         "1659",
         "0.4999991742107055",
         "0.499999040334404",
         "1.7854548905303506e-06",
         "1.338763014890887e-07"
        ],
        [
         "255",
         "0.49998864358518147",
         "0.4999884572115963",
         "2.289920322223563e-05",
         "1.8637358517592517e-07"
        ],
        [
         "1741",
         "0.4995990121857231",
         "0.4995988187468705",
         "0.0008021690674064204",
         "1.9343885260258986e-07"
        ],
        [
         "1415",
         "0.4999890837162564",
         "0.4999888836989485",
         "2.2032584795173087e-05",
         "2.0001730788310468e-07"
        ],
        [
         "1167",
         "0.49999130929462454",
         "0.4999911060051491",
         "1.7584700226311013e-05",
         "2.0328947542003206e-07"
        ],
        [
         "193",
         "0.499953393360738",
         "0.4999535993002288",
         "9.30073390332245e-05",
         "2.0593949079383833e-07"
        ],
        [
         "1681",
         "0.49996123154736705",
         "0.4999610212796038",
         "7.774717302916131e-05",
         "2.1026776325649976e-07"
        ],
        [
         "1322",
         "0.4999984112401421",
         "0.4999981961397682",
         "3.3926200896735655e-06",
         "2.1510037390148895e-07"
        ],
        [
         "1817",
         "0.4999953232070423",
         "0.49999509603377085",
         "9.58075918685483e-06",
         "2.2717327147203648e-07"
        ],
        [
         "3642",
         "0.4999968201977147",
         "0.49999704743020945",
         "6.132372075931775e-06",
         "2.2723249476497287e-07"
        ],
        [
         "1968",
         "0.49998852795327275",
         "0.49998876495883415",
         "2.2707087893125e-05",
         "2.3700556139871765e-07"
        ],
        [
         "3293",
         "0.49994797768279975",
         "0.4999482195536339",
         "0.00010380276356641953",
         "2.418708341389575e-07"
        ],
        [
         "2764",
         "0.4998450778597632",
         "0.4998453213310628",
         "0.0003096008091740424",
         "2.4347129962132286e-07"
        ],
        [
         "1739",
         "0.4999895493922066",
         "0.4999892848549712",
         "2.1165752822213515e-05",
         "2.645372353948794e-07"
        ],
        [
         "3371",
         "0.4999983330194307",
         "0.49999805802682473",
         "3.608953744572862e-06",
         "2.7499260596419717e-07"
        ],
        [
         "1620",
         "0.49997841059854997",
         "0.4999781327004328",
         "4.3456701017238336e-05",
         "2.7789811718470503e-07"
        ],
        [
         "1570",
         "0.49999405308987344",
         "0.4999937737669302",
         "1.2173143196292862e-05",
         "2.793229432196398e-07"
        ],
        [
         "3966",
         "0.49959087843624295",
         "0.4995905953880967",
         "0.0008185261756603101",
         "2.83048146254572e-07"
        ],
        [
         "391",
         "0.4999582477578383",
         "0.49995795720979014",
         "8.379503237148546e-05",
         "2.9054804817629787e-07"
        ],
        [
         "211",
         "0.49999502887382824",
         "0.4999953209774767",
         "9.650148695048928e-06",
         "2.921036484360151e-07"
        ],
        [
         "1069",
         "0.4998777964962018",
         "0.49987748379646446",
         "0.00024471970733371017",
         "3.126997373259499e-07"
        ],
        [
         "60",
         "0.49998091377937587",
         "0.4999812657696269",
         "3.7820450997265114e-05",
         "3.519902510573658e-07"
        ],
        [
         "1806",
         "0.49997597216161876",
         "0.4999756107725833",
         "4.841706579794438e-05",
         "3.613890354747795e-07"
        ],
        [
         "1936",
         "0.49999302204441015",
         "0.4999933871571952",
         "1.3590798394606324e-05",
         "3.65112785061239e-07"
        ],
        [
         "2327",
         "0.49998538184316355",
         "0.49998496075225446",
         "2.965740458195445e-05",
         "4.2109090908626357e-07"
        ],
        [
         "3602",
         "0.49996020554671894",
         "0.4999606328191089",
         "7.916163417220868e-05",
         "4.272723899689801e-07"
        ],
        [
         "1999",
         "0.49968292144237353",
         "0.499682491597391",
         "0.0006345869602354999",
         "4.2984498255238535e-07"
        ],
        [
         "1768",
         "0.4999959609895574",
         "0.49999523781386906",
         "8.801196573524708e-06",
         "7.23175688333022e-07"
        ],
        [
         "67",
         "0.49953110866258993",
         "0.4995318474763924",
         "0.000937043861017633",
         "7.388138024699842e-07"
        ],
        [
         "4401",
         "0.49868876644826604",
         "0.49868799173654205",
         "0.002623241815191925",
         "7.747117239942369e-07"
        ],
        [
         "2565",
         "0.49991805263286165",
         "0.4999190078305215",
         "0.00016293953661679768",
         "9.551976598576317e-07"
        ],
        [
         "261",
         "0.4684726992692199",
         "0.5308521333257996",
         "0.0006751674049805146",
         "0.0623794340565797"
        ],
        [
         "3588",
         "0.4349883885061504",
         "0.5585375773780997",
         "0.00647403411574992",
         "0.12354918887194932"
        ],
        [
         "4113",
         "0.5603085465345841",
         "0.4363694598545888",
         "0.0033219936108271255",
         "0.12393908667999526"
        ],
        [
         "3927",
         "0.5607966010438976",
         "0.43675163167926084",
         "0.0024517672768415767",
         "0.12404496936463677"
        ],
        [
         "3947",
         "0.43724773179633053",
         "0.5614375898902545",
         "0.001314678313414957",
         "0.12418985809392397"
        ],
        [
         "4135",
         "0.43725878650797",
         "0.5614527751615422",
         "0.001288438330487716",
         "0.12419398865357223"
        ],
        [
         "240",
         "0.4375557565932995",
         "0.5618326340400964",
         "0.0006116093666041134",
         "0.12427687744679694"
        ],
        [
         "1788",
         "0.5618335281217304",
         "0.4375562463075384",
         "0.0006102255707311772",
         "0.12427728181419195"
        ],
        [
         "3609",
         "0.5618546020127958",
         "0.43757368807408864",
         "0.0005717099131155993",
         "0.12428091393870716"
        ],
        [
         "4183",
         "0.4376176956132863",
         "0.5619123435118213",
         "0.00046996087489241037",
         "0.124294647898535"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 4407
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yes</th>\n",
       "      <th>no</th>\n",
       "      <th>other</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3569</th>\n",
       "      <td>4.998673e-01</td>\n",
       "      <td>4.998673e-01</td>\n",
       "      <td>2.653495e-04</td>\n",
       "      <td>3.803709e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>4.999913e-01</td>\n",
       "      <td>4.999912e-01</td>\n",
       "      <td>1.746399e-05</td>\n",
       "      <td>6.462529e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>4.999973e-01</td>\n",
       "      <td>4.999973e-01</td>\n",
       "      <td>5.371852e-06</td>\n",
       "      <td>6.573800e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>4.999896e-01</td>\n",
       "      <td>4.999895e-01</td>\n",
       "      <td>2.093080e-05</td>\n",
       "      <td>6.901112e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>4.999897e-01</td>\n",
       "      <td>4.999896e-01</td>\n",
       "      <td>2.069824e-05</td>\n",
       "      <td>8.234505e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>9.237450e-09</td>\n",
       "      <td>6.291377e-08</td>\n",
       "      <td>9.999999e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>8.592168e-10</td>\n",
       "      <td>7.365126e-08</td>\n",
       "      <td>9.999999e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>4.363463e-09</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>6.301088e-08</td>\n",
       "      <td>9.999999e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>2.335593e-09</td>\n",
       "      <td>5.015960e-08</td>\n",
       "      <td>9.999999e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.416609e-09</td>\n",
       "      <td>4.588376e-08</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4407 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               yes            no         other          diff\n",
       "3569  4.998673e-01  4.998673e-01  2.653495e-04  3.803709e-08\n",
       "1529  4.999913e-01  4.999912e-01  1.746399e-05  6.462529e-08\n",
       "3362  4.999973e-01  4.999973e-01  5.371852e-06  6.573800e-08\n",
       "1886  4.999896e-01  4.999895e-01  2.093080e-05  6.901112e-08\n",
       "2337  4.999897e-01  4.999896e-01  2.069824e-05  8.234505e-08\n",
       "...            ...           ...           ...           ...\n",
       "632   9.999999e-01  9.237450e-09  6.291377e-08  9.999999e-01\n",
       "449   9.999999e-01  8.592168e-10  7.365126e-08  9.999999e-01\n",
       "2914  4.363463e-09  9.999999e-01  6.301088e-08  9.999999e-01\n",
       "596   9.999999e-01  2.335593e-09  5.015960e-08  9.999999e-01\n",
       "3093  1.000000e+00  1.416609e-09  4.588376e-08  1.000000e+00\n",
       "\n",
       "[4407 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = all_models[\"GPT-4o\"][\"q2\"]\n",
    "df[\"diff\"] = np.abs(df[\"yes\"] - df[\"no\"])\n",
    "df.sort_values(\"diff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3caf5",
   "metadata": {},
   "source": [
    "## Calculate Individual-Level Commonsensicality of Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9718d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_answers(q_answers):\n",
    "    assert q_answers.columns[0].lower() == \"yes\"\n",
    "    assert q_answers.columns[1].lower() == \"no\"\n",
    "    if len(q_answers.columns) > 2:\n",
    "        assert q_answers.columns[2].lower() == \"other\"\n",
    "\n",
    "    # Remove the \"other\" answer and rescale so \"yes\" + \"no\" = 1\n",
    "    q_answers = q_answers.to_numpy()\n",
    "    other = q_answers[:, 2]\n",
    "    q_answers[:, 0] += other / 2\n",
    "    q_answers[:, 1] += other / 2\n",
    "    q_answers = q_answers[:, 0:2]\n",
    "    q_answers /= q_answers.sum(1, keepdims=True)\n",
    "\n",
    "    # Get the answer with the highest probability\n",
    "    q_answers = q_answers.argmax(axis=1)\n",
    "\n",
    "    # Revert the ordering so that No = 0, Yes = 1\n",
    "    # (Previously, no = 1, yes = 0)\n",
    "    q_answers = 1 - q_answers\n",
    "\n",
    "    return q_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931df0d",
   "metadata": {},
   "source": [
    "### The default way\n",
    "\n",
    "This version only uses human ratings in calculating the majority vote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c98392e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def compute_commonsensicality(answers, binary=False, return_ca=False):\n",
    "    q1_answers = answers[\"q1\"]\n",
    "    if not binary:\n",
    "        q1_answers = get_binary_answers(q1_answers)\n",
    "\n",
    "    q2_answers = answers[\"q2\"]\n",
    "    if not binary:\n",
    "        q2_answers = get_binary_answers(q2_answers)\n",
    "\n",
    "    consensus = accuracy_score(y_true=maj_i, y_pred=q1_answers)\n",
    "    awareness = accuracy_score(y_true=maj_i, y_pred=q2_answers)\n",
    "\n",
    "    commonsensicality = np.sqrt(consensus * awareness)\n",
    "\n",
    "    if return_ca:\n",
    "        return consensus, awareness, commonsensicality\n",
    "\n",
    "    return commonsensicality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86d48d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commonsensicality for all models\n",
    "all_model_comm = {}\n",
    "all_model_cons = {}\n",
    "all_model_awar = {}\n",
    "for model_name, model_answers in all_models.items():\n",
    "    con, awe, com = compute_commonsensicality(model_answers, return_ca=True)\n",
    "    all_model_comm[model_name] = com\n",
    "    all_model_awar[model_name] = awe\n",
    "    all_model_cons[model_name] = con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2aa1f29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "395f1ffa-33f4-40e5-b118-c1014935fc11",
       "rows": [
        [
         "Mixtral-8x22B",
         "0.8234163076893198"
        ],
        [
         "Mistral-Large",
         "0.8127496442363955"
        ],
        [
         "Qwen2-72B",
         "0.8114132219235464"
        ],
        [
         "Qwen2-57B",
         "0.8091532029671507"
        ],
        [
         "GPT-4-0409",
         "0.8057801400622315"
        ],
        [
         "Mistral-7B",
         "0.8043974789302533"
        ],
        [
         "Flan-T5-XXL",
         "0.8040455107032294"
        ],
        [
         "Qwen2-7B",
         "0.803690569453512"
        ],
        [
         "Falcon-180B",
         "0.7991786722950114"
        ],
        [
         "Gemini Pro 1.0",
         "0.7972476756282151"
        ],
        [
         "GPT-4-0125",
         "0.7842790319335812"
        ],
        [
         "Flan-T5-Large",
         "0.7689952871649802"
        ],
        [
         "GPT-3.5",
         "0.7684212590476057"
        ],
        [
         "Mixtral-8x7B",
         "0.7638864047193233"
        ],
        [
         "DBRX",
         "0.7628771057671772"
        ],
        [
         "GPT-5",
         "0.7567950184785917"
        ],
        [
         "Claude 3 Opus",
         "0.753990227866812"
        ],
        [
         "GPT-4o",
         "0.7516181113526245"
        ],
        [
         "Falcon-40B",
         "0.7505478974273945"
        ],
        [
         "LLaMA-2-7B",
         "0.7498768090446158"
        ],
        [
         "Qwen2-1.5B",
         "0.7459263376857646"
        ],
        [
         "LLaMA-3-70B",
         "0.7435300877159524"
        ],
        [
         "Flan-T5-XL",
         "0.7299690794610058"
        ],
        [
         "OLMo-7B",
         "0.7266146259156062"
        ],
        [
         "Gemma-7B",
         "0.7204670804790796"
        ],
        [
         "Qwen2-0.5B",
         "0.6677936692664421"
        ],
        [
         "Falcon-7B",
         "0.6633729441101213"
        ],
        [
         "Gemma-2B",
         "0.6588042055586673"
        ],
        [
         "LLaMA-2-70B",
         "0.6354487917444178"
        ],
        [
         "LLaMA-3-8B",
         "0.6168438804817648"
        ],
        [
         "Claude 3 Sonnet",
         "0.6154664387329493"
        ],
        [
         "Claude 3 Haiku",
         "0.6140118244621844"
        ],
        [
         "Flan-T5-Base",
         "0.5813097701101047"
        ],
        [
         "LLaMA-2-13B",
         "0.46496738615176286"
        ],
        [
         "Flan-T5-Small",
         "0.34172153371972364"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 35
       }
      },
      "text/plain": [
       "Mixtral-8x22B      0.823416\n",
       "Mistral-Large      0.812750\n",
       "Qwen2-72B          0.811413\n",
       "Qwen2-57B          0.809153\n",
       "GPT-4-0409         0.805780\n",
       "Mistral-7B         0.804397\n",
       "Flan-T5-XXL        0.804046\n",
       "Qwen2-7B           0.803691\n",
       "Falcon-180B        0.799179\n",
       "Gemini Pro 1.0     0.797248\n",
       "GPT-4-0125         0.784279\n",
       "Flan-T5-Large      0.768995\n",
       "GPT-3.5            0.768421\n",
       "Mixtral-8x7B       0.763886\n",
       "DBRX               0.762877\n",
       "GPT-5              0.756795\n",
       "Claude 3 Opus      0.753990\n",
       "GPT-4o             0.751618\n",
       "Falcon-40B         0.750548\n",
       "LLaMA-2-7B         0.749877\n",
       "Qwen2-1.5B         0.745926\n",
       "LLaMA-3-70B        0.743530\n",
       "Flan-T5-XL         0.729969\n",
       "OLMo-7B            0.726615\n",
       "Gemma-7B           0.720467\n",
       "Qwen2-0.5B         0.667794\n",
       "Falcon-7B          0.663373\n",
       "Gemma-2B           0.658804\n",
       "LLaMA-2-70B        0.635449\n",
       "LLaMA-3-8B         0.616844\n",
       "Claude 3 Sonnet    0.615466\n",
       "Claude 3 Haiku     0.614012\n",
       "Flan-T5-Base       0.581310\n",
       "LLaMA-2-13B        0.464967\n",
       "Flan-T5-Small      0.341722\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_model_cons_pd = pd.Series(all_model_cons)\n",
    "all_model_awar_pd = pd.Series(all_model_awar)\n",
    "all_model_comm_pd = pd.Series(all_model_comm)\n",
    "all_model_comm_pd.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd9e363",
   "metadata": {},
   "source": [
    "### The alternative way\n",
    "\n",
    "For each model, this version adds one more rating to every statement, so that the majority vote includes the model's own rating as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3d9d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def compute_commonsensicality_with_model(answers, binary=False, return_ca=False):\n",
    "    q1_answers = answers[\"q1\"].copy()\n",
    "    if not binary:\n",
    "        q1_answers = get_binary_answers(q1_answers)\n",
    "\n",
    "    q2_answers = answers[\"q2\"].copy()\n",
    "    if not binary:\n",
    "        q2_answers = get_binary_answers(q2_answers)\n",
    "\n",
    "    sum_ratings_per_statement_q1 = individual.sum(axis=1, skipna=True)\n",
    "    tot_ratings_per_statement_q1 = individual.notna().sum(axis=1, skipna=True)\n",
    "    sum_ratings_per_statement_q1 += q1_answers\n",
    "    tot_ratings_per_statement_q1 += 1\n",
    "    avg_ratings_per_statement_q1 = (\n",
    "        sum_ratings_per_statement_q1 / tot_ratings_per_statement_q1\n",
    "    )\n",
    "    maj_i_with_model = (avg_ratings_per_statement_q1 >= 0.5).astype(int)\n",
    "\n",
    "    consensus = accuracy_score(y_true=maj_i_with_model, y_pred=q1_answers)\n",
    "    awareness = accuracy_score(y_true=maj_i_with_model, y_pred=q2_answers)\n",
    "\n",
    "    commonsensicality = np.sqrt(consensus * awareness)\n",
    "\n",
    "    if return_ca:\n",
    "        return consensus, awareness, commonsensicality\n",
    "    return commonsensicality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be63d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commonsensicality for all models\n",
    "all_model_comm_with_model = {}\n",
    "all_model_cons_with_model = {}\n",
    "all_model_awar_with_model = {}\n",
    "for model_name, model_answers in all_models.items():\n",
    "    con, awe, com = compute_commonsensicality_with_model(model_answers, return_ca=True)\n",
    "    all_model_comm_with_model[model_name] = com\n",
    "    all_model_awar_with_model[model_name] = awe\n",
    "    all_model_cons_with_model[model_name] = con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7feffa33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "6313abba-cd79-49ca-900c-915f33e0e721",
       "rows": [
        [
         "Mixtral-8x22B",
         "0.835720311656551"
        ],
        [
         "Mistral-Large",
         "0.8272902676099382"
        ],
        [
         "Qwen2-72B",
         "0.8245892391500831"
        ],
        [
         "Qwen2-57B",
         "0.8227804046373832"
        ],
        [
         "Mistral-7B",
         "0.8186955141387561"
        ],
        [
         "Flan-T5-XXL",
         "0.8181260857538686"
        ],
        [
         "GPT-4-0409",
         "0.8178901076337688"
        ],
        [
         "Qwen2-7B",
         "0.8161912780092503"
        ],
        [
         "Falcon-180B",
         "0.8139714420916137"
        ],
        [
         "Gemini Pro 1.0",
         "0.8111363167038282"
        ],
        [
         "GPT-4-0125",
         "0.79678270130085"
        ],
        [
         "Flan-T5-Large",
         "0.7839641379338889"
        ],
        [
         "GPT-3.5",
         "0.7812731387520201"
        ],
        [
         "DBRX",
         "0.7750118612778889"
        ],
        [
         "Mixtral-8x7B",
         "0.7749071173457431"
        ],
        [
         "GPT-5",
         "0.7696599745175657"
        ],
        [
         "Claude 3 Opus",
         "0.7663432359632847"
        ],
        [
         "GPT-4o",
         "0.7644659259604403"
        ],
        [
         "LLaMA-2-7B",
         "0.7639705201732747"
        ],
        [
         "Falcon-40B",
         "0.7629481389382846"
        ],
        [
         "Qwen2-1.5B",
         "0.7583882298448079"
        ],
        [
         "LLaMA-3-70B",
         "0.7570269840122289"
        ],
        [
         "Flan-T5-XL",
         "0.7456181382263773"
        ],
        [
         "OLMo-7B",
         "0.7406553590152795"
        ],
        [
         "Gemma-7B",
         "0.7369863184290955"
        ],
        [
         "Qwen2-0.5B",
         "0.682994661852203"
        ],
        [
         "Falcon-7B",
         "0.6790299343310477"
        ],
        [
         "Gemma-2B",
         "0.6733394483403607"
        ],
        [
         "LLaMA-2-70B",
         "0.6489415276573169"
        ],
        [
         "LLaMA-3-8B",
         "0.6315671765045534"
        ],
        [
         "Claude 3 Sonnet",
         "0.6313553780769977"
        ],
        [
         "Claude 3 Haiku",
         "0.629512199174832"
        ],
        [
         "Flan-T5-Base",
         "0.5959019179860651"
        ],
        [
         "LLaMA-2-13B",
         "0.48127116992626195"
        ],
        [
         "Flan-T5-Small",
         "0.35828643840965124"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 35
       }
      },
      "text/plain": [
       "Mixtral-8x22B      0.835720\n",
       "Mistral-Large      0.827290\n",
       "Qwen2-72B          0.824589\n",
       "Qwen2-57B          0.822780\n",
       "Mistral-7B         0.818696\n",
       "Flan-T5-XXL        0.818126\n",
       "GPT-4-0409         0.817890\n",
       "Qwen2-7B           0.816191\n",
       "Falcon-180B        0.813971\n",
       "Gemini Pro 1.0     0.811136\n",
       "GPT-4-0125         0.796783\n",
       "Flan-T5-Large      0.783964\n",
       "GPT-3.5            0.781273\n",
       "DBRX               0.775012\n",
       "Mixtral-8x7B       0.774907\n",
       "GPT-5              0.769660\n",
       "Claude 3 Opus      0.766343\n",
       "GPT-4o             0.764466\n",
       "LLaMA-2-7B         0.763971\n",
       "Falcon-40B         0.762948\n",
       "Qwen2-1.5B         0.758388\n",
       "LLaMA-3-70B        0.757027\n",
       "Flan-T5-XL         0.745618\n",
       "OLMo-7B            0.740655\n",
       "Gemma-7B           0.736986\n",
       "Qwen2-0.5B         0.682995\n",
       "Falcon-7B          0.679030\n",
       "Gemma-2B           0.673339\n",
       "LLaMA-2-70B        0.648942\n",
       "LLaMA-3-8B         0.631567\n",
       "Claude 3 Sonnet    0.631355\n",
       "Claude 3 Haiku     0.629512\n",
       "Flan-T5-Base       0.595902\n",
       "LLaMA-2-13B        0.481271\n",
       "Flan-T5-Small      0.358286\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_model_cons_with_model_pd = pd.Series(all_model_cons_with_model)\n",
    "all_model_awar_with_model_pd = pd.Series(all_model_awar_with_model)\n",
    "all_model_comm_with_model_pd = pd.Series(all_model_comm_with_model)\n",
    "all_model_comm_with_model_pd.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb085a2",
   "metadata": {},
   "source": [
    "### Comparing results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1bfe142",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_order = [\n",
    "    \"Claude 3 Haiku\",\n",
    "    \"Claude 3 Sonnet\",\n",
    "    \"Claude 3 Opus\",\n",
    "    \"DBRX\",\n",
    "    \"Falcon-7B\",\n",
    "    \"Falcon-40B\",\n",
    "    \"Falcon-180B\",\n",
    "    \"Flan-T5-Small\",\n",
    "    \"Flan-T5-Base\",\n",
    "    \"Flan-T5-Large\",\n",
    "    \"Flan-T5-XL\",\n",
    "    \"Flan-T5-XXL\",\n",
    "    \"Gemma-2B\",\n",
    "    \"Gemma-7B\",\n",
    "    \"Gemini Pro 1.0\",\n",
    "    \"GPT-3.5\",\n",
    "    \"GPT-4-0125\",\n",
    "    \"GPT-4-0409\",\n",
    "    \"GPT-4o\",\n",
    "    \"GPT-5\",\n",
    "    \"LLaMA-2-7B\",\n",
    "    \"LLaMA-3-8B\",\n",
    "    \"LLaMA-2-13B\",\n",
    "    \"LLaMA-2-70B\",\n",
    "    \"LLaMA-3-70B\",\n",
    "    \"Mistral-7B\",\n",
    "    \"Mixtral-8x7B\",\n",
    "    \"Mixtral-8x22B\",\n",
    "    \"Mistral-Large\",\n",
    "    \"OLMo-7B\",\n",
    "    \"Qwen2-0.5B\",\n",
    "    \"Qwen2-1.5B\",\n",
    "    \"Qwen2-7B\",\n",
    "    \"Qwen2-57B\",\n",
    "    \"Qwen2-72B\",\n",
    "]\n",
    "model_comm_table = pd.DataFrame(\n",
    "    {\n",
    "        \"Consensus\": all_model_cons_pd[model_name_order],\n",
    "        \"Consensus (with model)\": all_model_cons_with_model_pd[model_name_order],\n",
    "        \"Awareness\": all_model_awar_pd[model_name_order],\n",
    "        \"Awareness (with model)\": all_model_awar_with_model_pd[model_name_order],\n",
    "        \"Commonsensicality\": all_model_comm_pd[model_name_order],\n",
    "        \"Commonsensicality (with model)\": all_model_comm_with_model_pd[\n",
    "            model_name_order\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d1a5a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_col(col):\n",
    "    col = col * 100\n",
    "    col_num = col.round(1)\n",
    "    col_ranking = col.rank(ascending=False, method=\"min\").astype(int)\n",
    "    col_str = col_num.astype(str) + \" (\" + col_ranking.astype(str) + \")\"\n",
    "    return col_str\n",
    "\n",
    "\n",
    "model_comm_table_with_ranking = model_comm_table.copy()\n",
    "\n",
    "for col in model_comm_table.columns:\n",
    "    model_comm_table_with_ranking[col] = process_col(model_comm_table[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf7992d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Consensus",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Consensus (with model)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Awareness",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Awareness (with model)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Commonsensicality",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Commonsensicality (with model)",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "7399f46b-aee0-412e-b29f-70cda7068cc9",
       "rows": [
        [
         "Claude 3 Haiku",
         "58.8 (31)",
         "60.5 (31)",
         "64.1 (30)",
         "65.5 (30)",
         "61.4 (32)",
         "63.0 (32)"
        ],
        [
         "Claude 3 Sonnet",
         "60.9 (30)",
         "62.6 (30)",
         "62.2 (31)",
         "63.7 (31)",
         "61.5 (31)",
         "63.1 (31)"
        ],
        [
         "Claude 3 Opus",
         "73.4 (19)",
         "75.1 (20)",
         "77.4 (15)",
         "78.2 (15)",
         "75.4 (17)",
         "76.6 (17)"
        ],
        [
         "DBRX",
         "73.7 (18)",
         "75.2 (18)",
         "79.0 (13)",
         "79.9 (13)",
         "76.3 (15)",
         "77.5 (14)"
        ],
        [
         "Falcon-7B",
         "66.6 (27)",
         "68.1 (27)",
         "66.1 (29)",
         "67.7 (28)",
         "66.3 (27)",
         "67.9 (27)"
        ],
        [
         "Falcon-40B",
         "73.0 (22)",
         "74.8 (22)",
         "77.2 (16)",
         "77.9 (17)",
         "75.1 (19)",
         "76.3 (20)"
        ],
        [
         "Falcon-180B",
         "78.6 (8)",
         "80.3 (8)",
         "81.3 (6)",
         "82.5 (5)",
         "79.9 (9)",
         "81.4 (9)"
        ],
        [
         "Flan-T5-Small",
         "34.4 (35)",
         "36.1 (35)",
         "33.9 (35)",
         "35.6 (35)",
         "34.2 (35)",
         "35.8 (35)"
        ],
        [
         "Flan-T5-Base",
         "56.8 (33)",
         "58.6 (33)",
         "59.5 (33)",
         "60.6 (33)",
         "58.1 (33)",
         "59.6 (33)"
        ],
        [
         "Flan-T5-Large",
         "77.3 (14)",
         "78.9 (14)",
         "76.5 (18)",
         "77.9 (16)",
         "76.9 (12)",
         "78.4 (12)"
        ],
        [
         "Flan-T5-XL",
         "73.3 (20)",
         "75.0 (21)",
         "72.7 (23)",
         "74.1 (23)",
         "73.0 (23)",
         "74.6 (23)"
        ],
        [
         "Flan-T5-XXL",
         "79.9 (6)",
         "81.6 (6)",
         "80.9 (9)",
         "82.1 (8)",
         "80.4 (7)",
         "81.8 (6)"
        ],
        [
         "Gemma-2B",
         "65.2 (29)",
         "66.8 (29)",
         "66.6 (26)",
         "67.9 (27)",
         "65.9 (28)",
         "67.3 (28)"
        ],
        [
         "Gemma-7B",
         "73.2 (21)",
         "75.1 (19)",
         "70.9 (25)",
         "72.3 (25)",
         "72.0 (25)",
         "73.7 (25)"
        ],
        [
         "Gemini Pro 1.0",
         "78.4 (9)",
         "80.0 (9)",
         "81.1 (7)",
         "82.2 (7)",
         "79.7 (10)",
         "81.1 (10)"
        ],
        [
         "GPT-3.5",
         "78.3 (10)",
         "80.0 (10)",
         "75.4 (20)",
         "76.3 (20)",
         "76.8 (13)",
         "78.1 (13)"
        ],
        [
         "GPT-4-0125",
         "77.6 (13)",
         "79.1 (13)",
         "79.2 (12)",
         "80.2 (12)",
         "78.4 (11)",
         "79.7 (11)"
        ],
        [
         "GPT-4-0409",
         "78.0 (11)",
         "79.4 (11)",
         "83.3 (2)",
         "84.2 (2)",
         "80.6 (5)",
         "81.8 (7)"
        ],
        [
         "GPT-4o",
         "72.5 (23)",
         "74.2 (23)",
         "77.9 (14)",
         "78.8 (14)",
         "75.2 (18)",
         "76.4 (18)"
        ],
        [
         "GPT-5",
         "71.9 (25)",
         "73.5 (25)",
         "79.6 (11)",
         "80.6 (11)",
         "75.7 (16)",
         "77.0 (16)"
        ],
        [
         "LLaMA-2-7B",
         "74.0 (17)",
         "75.6 (17)",
         "76.0 (19)",
         "77.2 (19)",
         "75.0 (20)",
         "76.4 (19)"
        ],
        [
         "LLaMA-3-8B",
         "57.2 (32)",
         "58.9 (32)",
         "66.5 (27)",
         "67.7 (28)",
         "61.7 (30)",
         "63.2 (30)"
        ],
        [
         "LLaMA-2-13B",
         "48.5 (34)",
         "50.3 (34)",
         "44.5 (34)",
         "46.1 (34)",
         "46.5 (34)",
         "48.1 (34)"
        ],
        [
         "LLaMA-2-70B",
         "65.7 (28)",
         "67.4 (28)",
         "61.4 (32)",
         "62.4 (32)",
         "63.5 (29)",
         "64.9 (29)"
        ],
        [
         "LLaMA-3-70B",
         "72.0 (24)",
         "73.6 (24)",
         "76.8 (17)",
         "77.8 (18)",
         "74.4 (22)",
         "75.7 (22)"
        ],
        [
         "Mistral-7B",
         "80.2 (5)",
         "81.7 (5)",
         "80.7 (10)",
         "82.1 (9)",
         "80.4 (6)",
         "81.9 (5)"
        ],
        [
         "Mixtral-8x7B",
         "77.8 (12)",
         "79.4 (12)",
         "75.0 (21)",
         "75.7 (21)",
         "76.4 (14)",
         "77.5 (15)"
        ],
        [
         "Mixtral-8x22B",
         "80.7 (1)",
         "82.2 (1)",
         "84.0 (1)",
         "84.9 (1)",
         "82.3 (1)",
         "83.6 (1)"
        ],
        [
         "Mistral-Large",
         "80.4 (4)",
         "82.0 (4)",
         "82.2 (3)",
         "83.4 (3)",
         "81.3 (2)",
         "82.7 (2)"
        ],
        [
         "OLMo-7B",
         "74.3 (16)",
         "75.9 (16)",
         "71.0 (24)",
         "72.3 (24)",
         "72.7 (24)",
         "74.1 (24)"
        ],
        [
         "Qwen2-0.5B",
         "67.1 (26)",
         "68.7 (26)",
         "66.5 (27)",
         "67.9 (26)",
         "66.8 (26)",
         "68.3 (26)"
        ],
        [
         "Qwen2-1.5B",
         "75.4 (15)",
         "76.8 (15)",
         "73.8 (22)",
         "74.9 (22)",
         "74.6 (21)",
         "75.8 (21)"
        ],
        [
         "Qwen2-7B",
         "79.7 (7)",
         "81.2 (7)",
         "81.1 (8)",
         "82.0 (10)",
         "80.4 (8)",
         "81.6 (8)"
        ],
        [
         "Qwen2-57B",
         "80.4 (3)",
         "82.1 (2)",
         "81.4 (5)",
         "82.4 (6)",
         "80.9 (4)",
         "82.3 (4)"
        ],
        [
         "Qwen2-72B",
         "80.5 (2)",
         "82.1 (3)",
         "81.8 (4)",
         "82.8 (4)",
         "81.1 (3)",
         "82.5 (3)"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 35
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consensus</th>\n",
       "      <th>Consensus (with model)</th>\n",
       "      <th>Awareness</th>\n",
       "      <th>Awareness (with model)</th>\n",
       "      <th>Commonsensicality</th>\n",
       "      <th>Commonsensicality (with model)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Claude 3 Haiku</th>\n",
       "      <td>58.8 (31)</td>\n",
       "      <td>60.5 (31)</td>\n",
       "      <td>64.1 (30)</td>\n",
       "      <td>65.5 (30)</td>\n",
       "      <td>61.4 (32)</td>\n",
       "      <td>63.0 (32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claude 3 Sonnet</th>\n",
       "      <td>60.9 (30)</td>\n",
       "      <td>62.6 (30)</td>\n",
       "      <td>62.2 (31)</td>\n",
       "      <td>63.7 (31)</td>\n",
       "      <td>61.5 (31)</td>\n",
       "      <td>63.1 (31)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Claude 3 Opus</th>\n",
       "      <td>73.4 (19)</td>\n",
       "      <td>75.1 (20)</td>\n",
       "      <td>77.4 (15)</td>\n",
       "      <td>78.2 (15)</td>\n",
       "      <td>75.4 (17)</td>\n",
       "      <td>76.6 (17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DBRX</th>\n",
       "      <td>73.7 (18)</td>\n",
       "      <td>75.2 (18)</td>\n",
       "      <td>79.0 (13)</td>\n",
       "      <td>79.9 (13)</td>\n",
       "      <td>76.3 (15)</td>\n",
       "      <td>77.5 (14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falcon-7B</th>\n",
       "      <td>66.6 (27)</td>\n",
       "      <td>68.1 (27)</td>\n",
       "      <td>66.1 (29)</td>\n",
       "      <td>67.7 (28)</td>\n",
       "      <td>66.3 (27)</td>\n",
       "      <td>67.9 (27)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falcon-40B</th>\n",
       "      <td>73.0 (22)</td>\n",
       "      <td>74.8 (22)</td>\n",
       "      <td>77.2 (16)</td>\n",
       "      <td>77.9 (17)</td>\n",
       "      <td>75.1 (19)</td>\n",
       "      <td>76.3 (20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falcon-180B</th>\n",
       "      <td>78.6 (8)</td>\n",
       "      <td>80.3 (8)</td>\n",
       "      <td>81.3 (6)</td>\n",
       "      <td>82.5 (5)</td>\n",
       "      <td>79.9 (9)</td>\n",
       "      <td>81.4 (9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flan-T5-Small</th>\n",
       "      <td>34.4 (35)</td>\n",
       "      <td>36.1 (35)</td>\n",
       "      <td>33.9 (35)</td>\n",
       "      <td>35.6 (35)</td>\n",
       "      <td>34.2 (35)</td>\n",
       "      <td>35.8 (35)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flan-T5-Base</th>\n",
       "      <td>56.8 (33)</td>\n",
       "      <td>58.6 (33)</td>\n",
       "      <td>59.5 (33)</td>\n",
       "      <td>60.6 (33)</td>\n",
       "      <td>58.1 (33)</td>\n",
       "      <td>59.6 (33)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flan-T5-Large</th>\n",
       "      <td>77.3 (14)</td>\n",
       "      <td>78.9 (14)</td>\n",
       "      <td>76.5 (18)</td>\n",
       "      <td>77.9 (16)</td>\n",
       "      <td>76.9 (12)</td>\n",
       "      <td>78.4 (12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flan-T5-XL</th>\n",
       "      <td>73.3 (20)</td>\n",
       "      <td>75.0 (21)</td>\n",
       "      <td>72.7 (23)</td>\n",
       "      <td>74.1 (23)</td>\n",
       "      <td>73.0 (23)</td>\n",
       "      <td>74.6 (23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flan-T5-XXL</th>\n",
       "      <td>79.9 (6)</td>\n",
       "      <td>81.6 (6)</td>\n",
       "      <td>80.9 (9)</td>\n",
       "      <td>82.1 (8)</td>\n",
       "      <td>80.4 (7)</td>\n",
       "      <td>81.8 (6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemma-2B</th>\n",
       "      <td>65.2 (29)</td>\n",
       "      <td>66.8 (29)</td>\n",
       "      <td>66.6 (26)</td>\n",
       "      <td>67.9 (27)</td>\n",
       "      <td>65.9 (28)</td>\n",
       "      <td>67.3 (28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemma-7B</th>\n",
       "      <td>73.2 (21)</td>\n",
       "      <td>75.1 (19)</td>\n",
       "      <td>70.9 (25)</td>\n",
       "      <td>72.3 (25)</td>\n",
       "      <td>72.0 (25)</td>\n",
       "      <td>73.7 (25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini Pro 1.0</th>\n",
       "      <td>78.4 (9)</td>\n",
       "      <td>80.0 (9)</td>\n",
       "      <td>81.1 (7)</td>\n",
       "      <td>82.2 (7)</td>\n",
       "      <td>79.7 (10)</td>\n",
       "      <td>81.1 (10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-3.5</th>\n",
       "      <td>78.3 (10)</td>\n",
       "      <td>80.0 (10)</td>\n",
       "      <td>75.4 (20)</td>\n",
       "      <td>76.3 (20)</td>\n",
       "      <td>76.8 (13)</td>\n",
       "      <td>78.1 (13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-4-0125</th>\n",
       "      <td>77.6 (13)</td>\n",
       "      <td>79.1 (13)</td>\n",
       "      <td>79.2 (12)</td>\n",
       "      <td>80.2 (12)</td>\n",
       "      <td>78.4 (11)</td>\n",
       "      <td>79.7 (11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-4-0409</th>\n",
       "      <td>78.0 (11)</td>\n",
       "      <td>79.4 (11)</td>\n",
       "      <td>83.3 (2)</td>\n",
       "      <td>84.2 (2)</td>\n",
       "      <td>80.6 (5)</td>\n",
       "      <td>81.8 (7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-4o</th>\n",
       "      <td>72.5 (23)</td>\n",
       "      <td>74.2 (23)</td>\n",
       "      <td>77.9 (14)</td>\n",
       "      <td>78.8 (14)</td>\n",
       "      <td>75.2 (18)</td>\n",
       "      <td>76.4 (18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-5</th>\n",
       "      <td>71.9 (25)</td>\n",
       "      <td>73.5 (25)</td>\n",
       "      <td>79.6 (11)</td>\n",
       "      <td>80.6 (11)</td>\n",
       "      <td>75.7 (16)</td>\n",
       "      <td>77.0 (16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLaMA-2-7B</th>\n",
       "      <td>74.0 (17)</td>\n",
       "      <td>75.6 (17)</td>\n",
       "      <td>76.0 (19)</td>\n",
       "      <td>77.2 (19)</td>\n",
       "      <td>75.0 (20)</td>\n",
       "      <td>76.4 (19)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLaMA-3-8B</th>\n",
       "      <td>57.2 (32)</td>\n",
       "      <td>58.9 (32)</td>\n",
       "      <td>66.5 (27)</td>\n",
       "      <td>67.7 (28)</td>\n",
       "      <td>61.7 (30)</td>\n",
       "      <td>63.2 (30)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLaMA-2-13B</th>\n",
       "      <td>48.5 (34)</td>\n",
       "      <td>50.3 (34)</td>\n",
       "      <td>44.5 (34)</td>\n",
       "      <td>46.1 (34)</td>\n",
       "      <td>46.5 (34)</td>\n",
       "      <td>48.1 (34)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLaMA-2-70B</th>\n",
       "      <td>65.7 (28)</td>\n",
       "      <td>67.4 (28)</td>\n",
       "      <td>61.4 (32)</td>\n",
       "      <td>62.4 (32)</td>\n",
       "      <td>63.5 (29)</td>\n",
       "      <td>64.9 (29)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLaMA-3-70B</th>\n",
       "      <td>72.0 (24)</td>\n",
       "      <td>73.6 (24)</td>\n",
       "      <td>76.8 (17)</td>\n",
       "      <td>77.8 (18)</td>\n",
       "      <td>74.4 (22)</td>\n",
       "      <td>75.7 (22)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral-7B</th>\n",
       "      <td>80.2 (5)</td>\n",
       "      <td>81.7 (5)</td>\n",
       "      <td>80.7 (10)</td>\n",
       "      <td>82.1 (9)</td>\n",
       "      <td>80.4 (6)</td>\n",
       "      <td>81.9 (5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixtral-8x7B</th>\n",
       "      <td>77.8 (12)</td>\n",
       "      <td>79.4 (12)</td>\n",
       "      <td>75.0 (21)</td>\n",
       "      <td>75.7 (21)</td>\n",
       "      <td>76.4 (14)</td>\n",
       "      <td>77.5 (15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixtral-8x22B</th>\n",
       "      <td>80.7 (1)</td>\n",
       "      <td>82.2 (1)</td>\n",
       "      <td>84.0 (1)</td>\n",
       "      <td>84.9 (1)</td>\n",
       "      <td>82.3 (1)</td>\n",
       "      <td>83.6 (1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral-Large</th>\n",
       "      <td>80.4 (4)</td>\n",
       "      <td>82.0 (4)</td>\n",
       "      <td>82.2 (3)</td>\n",
       "      <td>83.4 (3)</td>\n",
       "      <td>81.3 (2)</td>\n",
       "      <td>82.7 (2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLMo-7B</th>\n",
       "      <td>74.3 (16)</td>\n",
       "      <td>75.9 (16)</td>\n",
       "      <td>71.0 (24)</td>\n",
       "      <td>72.3 (24)</td>\n",
       "      <td>72.7 (24)</td>\n",
       "      <td>74.1 (24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2-0.5B</th>\n",
       "      <td>67.1 (26)</td>\n",
       "      <td>68.7 (26)</td>\n",
       "      <td>66.5 (27)</td>\n",
       "      <td>67.9 (26)</td>\n",
       "      <td>66.8 (26)</td>\n",
       "      <td>68.3 (26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2-1.5B</th>\n",
       "      <td>75.4 (15)</td>\n",
       "      <td>76.8 (15)</td>\n",
       "      <td>73.8 (22)</td>\n",
       "      <td>74.9 (22)</td>\n",
       "      <td>74.6 (21)</td>\n",
       "      <td>75.8 (21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2-7B</th>\n",
       "      <td>79.7 (7)</td>\n",
       "      <td>81.2 (7)</td>\n",
       "      <td>81.1 (8)</td>\n",
       "      <td>82.0 (10)</td>\n",
       "      <td>80.4 (8)</td>\n",
       "      <td>81.6 (8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2-57B</th>\n",
       "      <td>80.4 (3)</td>\n",
       "      <td>82.1 (2)</td>\n",
       "      <td>81.4 (5)</td>\n",
       "      <td>82.4 (6)</td>\n",
       "      <td>80.9 (4)</td>\n",
       "      <td>82.3 (4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2-72B</th>\n",
       "      <td>80.5 (2)</td>\n",
       "      <td>82.1 (3)</td>\n",
       "      <td>81.8 (4)</td>\n",
       "      <td>82.8 (4)</td>\n",
       "      <td>81.1 (3)</td>\n",
       "      <td>82.5 (3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Consensus Consensus (with model)  Awareness  \\\n",
       "Claude 3 Haiku   58.8 (31)              60.5 (31)  64.1 (30)   \n",
       "Claude 3 Sonnet  60.9 (30)              62.6 (30)  62.2 (31)   \n",
       "Claude 3 Opus    73.4 (19)              75.1 (20)  77.4 (15)   \n",
       "DBRX             73.7 (18)              75.2 (18)  79.0 (13)   \n",
       "Falcon-7B        66.6 (27)              68.1 (27)  66.1 (29)   \n",
       "Falcon-40B       73.0 (22)              74.8 (22)  77.2 (16)   \n",
       "Falcon-180B       78.6 (8)               80.3 (8)   81.3 (6)   \n",
       "Flan-T5-Small    34.4 (35)              36.1 (35)  33.9 (35)   \n",
       "Flan-T5-Base     56.8 (33)              58.6 (33)  59.5 (33)   \n",
       "Flan-T5-Large    77.3 (14)              78.9 (14)  76.5 (18)   \n",
       "Flan-T5-XL       73.3 (20)              75.0 (21)  72.7 (23)   \n",
       "Flan-T5-XXL       79.9 (6)               81.6 (6)   80.9 (9)   \n",
       "Gemma-2B         65.2 (29)              66.8 (29)  66.6 (26)   \n",
       "Gemma-7B         73.2 (21)              75.1 (19)  70.9 (25)   \n",
       "Gemini Pro 1.0    78.4 (9)               80.0 (9)   81.1 (7)   \n",
       "GPT-3.5          78.3 (10)              80.0 (10)  75.4 (20)   \n",
       "GPT-4-0125       77.6 (13)              79.1 (13)  79.2 (12)   \n",
       "GPT-4-0409       78.0 (11)              79.4 (11)   83.3 (2)   \n",
       "GPT-4o           72.5 (23)              74.2 (23)  77.9 (14)   \n",
       "GPT-5            71.9 (25)              73.5 (25)  79.6 (11)   \n",
       "LLaMA-2-7B       74.0 (17)              75.6 (17)  76.0 (19)   \n",
       "LLaMA-3-8B       57.2 (32)              58.9 (32)  66.5 (27)   \n",
       "LLaMA-2-13B      48.5 (34)              50.3 (34)  44.5 (34)   \n",
       "LLaMA-2-70B      65.7 (28)              67.4 (28)  61.4 (32)   \n",
       "LLaMA-3-70B      72.0 (24)              73.6 (24)  76.8 (17)   \n",
       "Mistral-7B        80.2 (5)               81.7 (5)  80.7 (10)   \n",
       "Mixtral-8x7B     77.8 (12)              79.4 (12)  75.0 (21)   \n",
       "Mixtral-8x22B     80.7 (1)               82.2 (1)   84.0 (1)   \n",
       "Mistral-Large     80.4 (4)               82.0 (4)   82.2 (3)   \n",
       "OLMo-7B          74.3 (16)              75.9 (16)  71.0 (24)   \n",
       "Qwen2-0.5B       67.1 (26)              68.7 (26)  66.5 (27)   \n",
       "Qwen2-1.5B       75.4 (15)              76.8 (15)  73.8 (22)   \n",
       "Qwen2-7B          79.7 (7)               81.2 (7)   81.1 (8)   \n",
       "Qwen2-57B         80.4 (3)               82.1 (2)   81.4 (5)   \n",
       "Qwen2-72B         80.5 (2)               82.1 (3)   81.8 (4)   \n",
       "\n",
       "                Awareness (with model) Commonsensicality  \\\n",
       "Claude 3 Haiku               65.5 (30)         61.4 (32)   \n",
       "Claude 3 Sonnet              63.7 (31)         61.5 (31)   \n",
       "Claude 3 Opus                78.2 (15)         75.4 (17)   \n",
       "DBRX                         79.9 (13)         76.3 (15)   \n",
       "Falcon-7B                    67.7 (28)         66.3 (27)   \n",
       "Falcon-40B                   77.9 (17)         75.1 (19)   \n",
       "Falcon-180B                   82.5 (5)          79.9 (9)   \n",
       "Flan-T5-Small                35.6 (35)         34.2 (35)   \n",
       "Flan-T5-Base                 60.6 (33)         58.1 (33)   \n",
       "Flan-T5-Large                77.9 (16)         76.9 (12)   \n",
       "Flan-T5-XL                   74.1 (23)         73.0 (23)   \n",
       "Flan-T5-XXL                   82.1 (8)          80.4 (7)   \n",
       "Gemma-2B                     67.9 (27)         65.9 (28)   \n",
       "Gemma-7B                     72.3 (25)         72.0 (25)   \n",
       "Gemini Pro 1.0                82.2 (7)         79.7 (10)   \n",
       "GPT-3.5                      76.3 (20)         76.8 (13)   \n",
       "GPT-4-0125                   80.2 (12)         78.4 (11)   \n",
       "GPT-4-0409                    84.2 (2)          80.6 (5)   \n",
       "GPT-4o                       78.8 (14)         75.2 (18)   \n",
       "GPT-5                        80.6 (11)         75.7 (16)   \n",
       "LLaMA-2-7B                   77.2 (19)         75.0 (20)   \n",
       "LLaMA-3-8B                   67.7 (28)         61.7 (30)   \n",
       "LLaMA-2-13B                  46.1 (34)         46.5 (34)   \n",
       "LLaMA-2-70B                  62.4 (32)         63.5 (29)   \n",
       "LLaMA-3-70B                  77.8 (18)         74.4 (22)   \n",
       "Mistral-7B                    82.1 (9)          80.4 (6)   \n",
       "Mixtral-8x7B                 75.7 (21)         76.4 (14)   \n",
       "Mixtral-8x22B                 84.9 (1)          82.3 (1)   \n",
       "Mistral-Large                 83.4 (3)          81.3 (2)   \n",
       "OLMo-7B                      72.3 (24)         72.7 (24)   \n",
       "Qwen2-0.5B                   67.9 (26)         66.8 (26)   \n",
       "Qwen2-1.5B                   74.9 (22)         74.6 (21)   \n",
       "Qwen2-7B                     82.0 (10)          80.4 (8)   \n",
       "Qwen2-57B                     82.4 (6)          80.9 (4)   \n",
       "Qwen2-72B                     82.8 (4)          81.1 (3)   \n",
       "\n",
       "                Commonsensicality (with model)  \n",
       "Claude 3 Haiku                       63.0 (32)  \n",
       "Claude 3 Sonnet                      63.1 (31)  \n",
       "Claude 3 Opus                        76.6 (17)  \n",
       "DBRX                                 77.5 (14)  \n",
       "Falcon-7B                            67.9 (27)  \n",
       "Falcon-40B                           76.3 (20)  \n",
       "Falcon-180B                           81.4 (9)  \n",
       "Flan-T5-Small                        35.8 (35)  \n",
       "Flan-T5-Base                         59.6 (33)  \n",
       "Flan-T5-Large                        78.4 (12)  \n",
       "Flan-T5-XL                           74.6 (23)  \n",
       "Flan-T5-XXL                           81.8 (6)  \n",
       "Gemma-2B                             67.3 (28)  \n",
       "Gemma-7B                             73.7 (25)  \n",
       "Gemini Pro 1.0                       81.1 (10)  \n",
       "GPT-3.5                              78.1 (13)  \n",
       "GPT-4-0125                           79.7 (11)  \n",
       "GPT-4-0409                            81.8 (7)  \n",
       "GPT-4o                               76.4 (18)  \n",
       "GPT-5                                77.0 (16)  \n",
       "LLaMA-2-7B                           76.4 (19)  \n",
       "LLaMA-3-8B                           63.2 (30)  \n",
       "LLaMA-2-13B                          48.1 (34)  \n",
       "LLaMA-2-70B                          64.9 (29)  \n",
       "LLaMA-3-70B                          75.7 (22)  \n",
       "Mistral-7B                            81.9 (5)  \n",
       "Mixtral-8x7B                         77.5 (15)  \n",
       "Mixtral-8x22B                         83.6 (1)  \n",
       "Mistral-Large                         82.7 (2)  \n",
       "OLMo-7B                              74.1 (24)  \n",
       "Qwen2-0.5B                           68.3 (26)  \n",
       "Qwen2-1.5B                           75.8 (21)  \n",
       "Qwen2-7B                              81.6 (8)  \n",
       "Qwen2-57B                             82.3 (4)  \n",
       "Qwen2-72B                             82.5 (3)  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comm_table_with_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0040937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllll}\n",
      "\\toprule\n",
      " & Consensus & Consensus (with model) & Awareness & Awareness (with model) & Commonsensicality & Commonsensicality (with model) \\\\\n",
      "\\midrule\n",
      "Claude 3 Haiku & 58.8 (31) & 60.5 (31) & 64.1 (30) & 65.5 (30) & 61.4 (32) & 63.0 (32) \\\\\n",
      "Claude 3 Sonnet & 60.9 (30) & 62.6 (30) & 62.2 (31) & 63.7 (31) & 61.5 (31) & 63.1 (31) \\\\\n",
      "Claude 3 Opus & 73.4 (19) & 75.1 (20) & 77.4 (15) & 78.2 (15) & 75.4 (17) & 76.6 (17) \\\\\n",
      "DBRX & 73.7 (18) & 75.2 (18) & 79.0 (13) & 79.9 (13) & 76.3 (15) & 77.5 (14) \\\\\n",
      "Falcon-7B & 66.6 (27) & 68.1 (27) & 66.1 (29) & 67.7 (28) & 66.3 (27) & 67.9 (27) \\\\\n",
      "Falcon-40B & 73.0 (22) & 74.8 (22) & 77.2 (16) & 77.9 (17) & 75.1 (19) & 76.3 (20) \\\\\n",
      "Falcon-180B & 78.6 (8) & 80.3 (8) & 81.3 (6) & 82.5 (5) & 79.9 (9) & 81.4 (9) \\\\\n",
      "Flan-T5-Small & 34.4 (35) & 36.1 (35) & 33.9 (35) & 35.6 (35) & 34.2 (35) & 35.8 (35) \\\\\n",
      "Flan-T5-Base & 56.8 (33) & 58.6 (33) & 59.5 (33) & 60.6 (33) & 58.1 (33) & 59.6 (33) \\\\\n",
      "Flan-T5-Large & 77.3 (14) & 78.9 (14) & 76.5 (18) & 77.9 (16) & 76.9 (12) & 78.4 (12) \\\\\n",
      "Flan-T5-XL & 73.3 (20) & 75.0 (21) & 72.7 (23) & 74.1 (23) & 73.0 (23) & 74.6 (23) \\\\\n",
      "Flan-T5-XXL & 79.9 (6) & 81.6 (6) & 80.9 (9) & 82.1 (8) & 80.4 (7) & 81.8 (6) \\\\\n",
      "Gemma-2B & 65.2 (29) & 66.8 (29) & 66.6 (26) & 67.9 (27) & 65.9 (28) & 67.3 (28) \\\\\n",
      "Gemma-7B & 73.2 (21) & 75.1 (19) & 70.9 (25) & 72.3 (25) & 72.0 (25) & 73.7 (25) \\\\\n",
      "Gemini Pro 1.0 & 78.4 (9) & 80.0 (9) & 81.1 (7) & 82.2 (7) & 79.7 (10) & 81.1 (10) \\\\\n",
      "GPT-3.5 & 78.3 (10) & 80.0 (10) & 75.4 (20) & 76.3 (20) & 76.8 (13) & 78.1 (13) \\\\\n",
      "GPT-4-0125 & 77.6 (13) & 79.1 (13) & 79.2 (12) & 80.2 (12) & 78.4 (11) & 79.7 (11) \\\\\n",
      "GPT-4-0409 & 78.0 (11) & 79.4 (11) & 83.3 (2) & 84.2 (2) & 80.6 (5) & 81.8 (7) \\\\\n",
      "GPT-4o & 72.5 (23) & 74.2 (23) & 77.9 (14) & 78.8 (14) & 75.2 (18) & 76.4 (18) \\\\\n",
      "GPT-5 & 71.9 (25) & 73.5 (25) & 79.6 (11) & 80.6 (11) & 75.7 (16) & 77.0 (16) \\\\\n",
      "LLaMA-2-7B & 74.0 (17) & 75.6 (17) & 76.0 (19) & 77.2 (19) & 75.0 (20) & 76.4 (19) \\\\\n",
      "LLaMA-3-8B & 57.2 (32) & 58.9 (32) & 66.5 (27) & 67.7 (28) & 61.7 (30) & 63.2 (30) \\\\\n",
      "LLaMA-2-13B & 48.5 (34) & 50.3 (34) & 44.5 (34) & 46.1 (34) & 46.5 (34) & 48.1 (34) \\\\\n",
      "LLaMA-2-70B & 65.7 (28) & 67.4 (28) & 61.4 (32) & 62.4 (32) & 63.5 (29) & 64.9 (29) \\\\\n",
      "LLaMA-3-70B & 72.0 (24) & 73.6 (24) & 76.8 (17) & 77.8 (18) & 74.4 (22) & 75.7 (22) \\\\\n",
      "Mistral-7B & 80.2 (5) & 81.7 (5) & 80.7 (10) & 82.1 (9) & 80.4 (6) & 81.9 (5) \\\\\n",
      "Mixtral-8x7B & 77.8 (12) & 79.4 (12) & 75.0 (21) & 75.7 (21) & 76.4 (14) & 77.5 (15) \\\\\n",
      "Mixtral-8x22B & 80.7 (1) & 82.2 (1) & 84.0 (1) & 84.9 (1) & 82.3 (1) & 83.6 (1) \\\\\n",
      "Mistral-Large & 80.4 (4) & 82.0 (4) & 82.2 (3) & 83.4 (3) & 81.3 (2) & 82.7 (2) \\\\\n",
      "OLMo-7B & 74.3 (16) & 75.9 (16) & 71.0 (24) & 72.3 (24) & 72.7 (24) & 74.1 (24) \\\\\n",
      "Qwen2-0.5B & 67.1 (26) & 68.7 (26) & 66.5 (27) & 67.9 (26) & 66.8 (26) & 68.3 (26) \\\\\n",
      "Qwen2-1.5B & 75.4 (15) & 76.8 (15) & 73.8 (22) & 74.9 (22) & 74.6 (21) & 75.8 (21) \\\\\n",
      "Qwen2-7B & 79.7 (7) & 81.2 (7) & 81.1 (8) & 82.0 (10) & 80.4 (8) & 81.6 (8) \\\\\n",
      "Qwen2-57B & 80.4 (3) & 82.1 (2) & 81.4 (5) & 82.4 (6) & 80.9 (4) & 82.3 (4) \\\\\n",
      "Qwen2-72B & 80.5 (2) & 82.1 (3) & 81.8 (4) & 82.8 (4) & 81.1 (3) & 82.5 (3) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_comm_table_with_ranking.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22101097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation for Consensus: 0.9989 (p-value: 2.6169541956072737e-45)\n",
      "Spearman correlation for Awareness: 0.9974 (p-value: 2.6392264837837585e-39)\n",
      "Spearman correlation for Commonsensicality: 0.9986 (p-value: 1.0373418768072296e-43)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "cols = [\"Consensus\", \"Awareness\", \"Commonsensicality\"]\n",
    "for col in cols:\n",
    "    col_data = model_comm_table[col]\n",
    "    col_data_with_model = model_comm_table[f\"{col} (with model)\"]\n",
    "    corr, p_value = spearmanr(col_data, col_data_with_model)\n",
    "    print(f\"Spearman correlation for {col}: {corr:.4f} (p-value: {p_value})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9ae6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commonsense-llm-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
